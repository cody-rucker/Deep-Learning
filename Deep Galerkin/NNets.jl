using Flux
using ForwardDiff
using Zygote

# ReLu activation for rectified linear units
#   relu(x) = max(0, x)
#   relu'(x) = 0    x<0
#              1    x‚âß0
œÅ(x) = max(0, x)

function dœÅ(x)
    if x < 0
        return 0
    else
        return x^2
    end
end

function ddœÅ(x)
    if x<0
        return 0
    else
        return 2*x
    end
end
# symbol for quick swapping of activation functions
œÜ = œÉ
dœÜ = œÉ'
ddœÜ = œÉ''


# create type to hold adaptive moment estimation parameters
"""
    Adam(N, M)
Defines a composite type to store a neural networks' vital parameters.

In addition to storing weight or bias information, this type also stores necesary
parameters for adaptive moment estimation.

# Arguments
- `N::Integer`: the desired input dimension.
- `M::Integer`: the desired output dimension.

# Examples
```jldoctest
julia> W = Adam(5, 1)
```
"""
mutable struct Adam
  Œ∏::AbstractArray     # Parameter array
  ‚àá::AbstractArray     # Gradient w.r.t Œ∏
  m::AbstractArray     # First moment
  v::AbstractArray     # Second moment
  Œ≤‚ÇÅ::Float64                   # Exp. decay first moment
  Œ≤‚ÇÇ::Float64                   # Exp. decay second moment
  Œ±::Float64                    # Step size
  œµ::Float64                    # Epsilon for stability
  t::Int                        # Time step (iteration)

  function Adam(N::Int64, M::Int64)
      Œ∏ = rand(N, M)
      ‚àá = rand(N, M)
      m = zeros(size(Œ∏))
      v = zeros(size(Œ∏))
      Œ≤‚ÇÅ= 0.91
      Œ≤‚ÇÇ= 0.9991
      Œ± = 0.0089
      œµ = 1e-8
      t = 0
    new(Œ∏, ‚àá, m, v, Œ≤‚ÇÅ, Œ≤‚ÇÇ, Œ±, œµ, t)
  end

end


"""

    NeuralNet(hidden_dim, out_dim, spatial_dim)

Defines a trainable neural network which accepts an input of size `spatial_dim`
with `hidden_dim` neurons in each layer and `out_dim` entries in its output.

Weight and Bias parameters are stored as the composite Adam type to prepare the
NN for adaptive moment estimation while training.

# Arguments
- `hidden_dim::Integer`: the desired hidden dimension.
- `out_dim::Integer`: the desired output dimension.
- `spatial_dim::Integer`: the number of inputs that the NN accepts.

# Examples
```jldoctest
julia> u = NeuralNet(5,1)
julia> u(1.0, 2.0)
1√ó1 Array{Float64,2}:
 3.338606039920986
```
"""
mutable struct NeuralNet
    W‚Çì::Adam
    W·µß::Adam
    Wùëß::Adam
    b‚ÇÅ::Adam
    W‚ÇÇ::Adam
    b‚ÇÇ::Adam
    W‚ÇÉ::Adam
    b‚ÇÉ::Adam

    Œ†::Tuple
    œÄ::Flux.Params

    function NeuralNet(hidden_dims::Integer, out_dims::Integer, spatial_dim::Integer=1)
        W‚Çì = Adam(hidden_dims, 1)
        W·µß = Adam(hidden_dims, 1)
        b‚ÇÅ = Adam(hidden_dims, 1)
        W‚ÇÇ = Adam(hidden_dims, hidden_dims)
        b‚ÇÇ = Adam(hidden_dims, 1)
        W‚ÇÉ = Adam(out_dims, hidden_dims)
        b‚ÇÉ = Adam(out_dims, 1)

        if spatial_dim == 1
            w·µß = Adam(1,1)
            Wùëß = Adam(1,1)
            Œ† = (W‚Çì, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, W‚ÇÉ, b‚ÇÉ)
            œÄ = Flux.params(W‚Çì.Œ∏, b‚ÇÅ.Œ∏, W‚ÇÇ.Œ∏, b‚ÇÇ.Œ∏, W‚ÇÉ.Œ∏, b‚ÇÉ.Œ∏)

        elseif spatial_dim ==2
            W·µß = Adam(hidden_dims, 1)
            Wùëß = Adam(1,1)
            Œ† = (W‚Çì, W·µß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, W‚ÇÉ, b‚ÇÉ)
            œÄ = Flux.params(W‚Çì.Œ∏, W·µß.Œ∏, b‚ÇÅ.Œ∏, W‚ÇÇ.Œ∏, b‚ÇÇ.Œ∏, W‚ÇÉ.Œ∏, b‚ÇÉ.Œ∏)

        elseif spatial_dim == 3
            W·µß = Adam(hidden_dims, 1)
            Wùëß = Adam(hidden_dims, 1)
            Œ† = (W‚Çì, W·µß, Wùëß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, W‚ÇÉ, b‚ÇÉ)
            œÄ = Flux.params(W‚Çì.Œ∏, W·µß.Œ∏, Wùëß.Œ∏, b‚ÇÅ.Œ∏, W‚ÇÇ.Œ∏, b‚ÇÇ.Œ∏, W‚ÇÉ.Œ∏, b‚ÇÉ.Œ∏)
        end

        new(W‚Çì, W·µß, Wùëß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, W‚ÇÉ, b‚ÇÉ, Œ†, œÄ);
    end
end

# define a forward-pass rule for each different input size
(u::NeuralNet)(x) = u.W‚ÇÉ.Œ∏ * œÜ.(
                     u.W‚ÇÇ.Œ∏ * œÜ.(
                     u.W‚Çì.Œ∏*x .+ u.b‚ÇÅ.Œ∏) .+ u.b‚ÇÇ.Œ∏) .+ u.b‚ÇÉ.Œ∏

(u::NeuralNet)(x, y) = u.W‚ÇÉ.Œ∏ * œÜ.(
                        u.W‚ÇÇ.Œ∏ * œÜ.(
                        u.W‚Çì.Œ∏*x + u.W·µß.Œ∏*y .+ u.b‚ÇÅ.Œ∏) .+ u.b‚ÇÇ.Œ∏) .+ u.b‚ÇÉ.Œ∏ ;

(u::NeuralNet)(x, y, t) = u.W‚ÇÉ.Œ∏ * œÜ.(
                        u.W‚ÇÇ.Œ∏ * œÜ.(
                        u.W‚Çì.Œ∏*x + u.W·µß.Œ∏*y + u.Wùëß.Œ∏*t .+ u.b‚ÇÅ.Œ∏) .+ u.b‚ÇÇ.Œ∏) .+ u.b‚ÇÉ.Œ∏ ;

"""

    FirstNetDerivative(u, "x")

Defines a neural network which inherits weight and bias parameters from `u` and
outputs the derivative of `u` with respect to `"x"`.

# Arguments
- `u::NeuralNet`: neural net to be differentiated.
- `"x"::String`: which variable we are differentiating with respect to. "x" and "y" are both viable inputs.

# Examples
```jldoctest
julia> u = NeuralNet(5,1)
julia> u‚Çì = FirstNetDerivative(u, "x")
julia> u‚Çì(1.0, 2.0)
1√ó1 Array{Float64,2}:
 0.027888484050549646
```
"""
mutable struct FirstNetDerivative
    W‚Çì::Adam
    W·µß::Adam
    Wùëß::Adam
    b‚ÇÅ::Adam
    W‚ÇÇ::Adam
    b‚ÇÇ::Adam
    W‚ÇÉ::Adam
    b‚ÇÉ::Adam

    dŒæ::AbstractArray

    function FirstNetDerivative(u::NeuralNet, d)
        W‚Çì = u.W‚Çì
        W·µß = u.W·µß
        Wùëß = u.Wùëß
        b‚ÇÅ = u.b‚ÇÅ
        W‚ÇÇ = u.W‚ÇÇ
        b‚ÇÇ = u.b‚ÇÇ
        W‚ÇÉ = u.W‚ÇÉ
        b‚ÇÉ = u.b‚ÇÉ

        if d == "x‚ÇÅ"
            dŒæ = W‚Çì.Œ∏

        elseif d == "x‚ÇÇ"
            dŒæ = W·µß.Œ∏

        elseif d == "x‚ÇÉ"
            dŒæ = Wùëß.Œ∏
        else
            print("Must specify x‚ÇÅ. x‚ÇÇ, or x‚ÇÉ as a string literal argument.")
        end

        new(W‚Çì, W·µß, Wùëß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, W‚ÇÉ, b‚ÇÉ, dŒæ)
    end


end

# supply a derivative computation for each input size
(u::FirstNetDerivative)(x) = u.W‚ÇÉ.Œ∏ * (dœÜ.(u.W‚ÇÇ.Œ∏ * œÜ.( u.W‚Çì.Œ∏*x .+ u.b‚ÇÅ.Œ∏) .+ u.b‚ÇÇ.Œ∏) .*
                  (u.W‚ÇÇ.Œ∏ * (dœÜ.( u.W‚Çì.Œ∏*x .+ u.b‚ÇÅ.Œ∏) .* u.dŒæ )))

(u::FirstNetDerivative)(x, y) = u.W‚ÇÉ.Œ∏ * (dœÜ.(u.W‚ÇÇ.Œ∏ * œÜ.( u.W‚Çì.Œ∏*x .+ u.W·µß.Œ∏*y .+ u.b‚ÇÅ.Œ∏) .+ u.b‚ÇÇ.Œ∏) .*
                  (u.W‚ÇÇ.Œ∏ * (dœÜ.( u.W‚Çì.Œ∏*x .+ u.W·µß.Œ∏*y .+ u.b‚ÇÅ.Œ∏) .* u.dŒæ )))

(u::FirstNetDerivative)(x, y, t) = u.W‚ÇÉ.Œ∏ * (dœÜ.(u.W‚ÇÇ.Œ∏ * œÜ.( u.W‚Çì.Œ∏*x .+ u.W·µß.Œ∏*y  .+ u.Wùëß.Œ∏*t .+ u.b‚ÇÅ.Œ∏) .+ u.b‚ÇÇ.Œ∏) .*
                (u.W‚ÇÇ.Œ∏ * (dœÜ.( u.W‚Çì.Œ∏*x .+ u.W·µß.Œ∏*y .+ u.Wùëß.Œ∏*t .+ u.b‚ÇÅ.Œ∏) .* u.dŒæ )))


"""

  SecondNetDerivative(u, "x", "y")

Defines a neural network which inherits weight and bias parameters from `u` and
outputs the second derivative of `u`. `"x"` and `"y"` are valid inputs allowing
you to specify any second-order derivative of `u`.

# Arguments
- `u::NeuralNet`: neural net to be differentiated.
- `"x"::String`: specifies a derivative w.r.t first input variable.
- `"y"::String`: specifies a derivative w.r.t second input variable.

# Examples
```jldoctest
julia> u = NeuralNet(5,1)
julia> u‚Çì·µß = SecondNetDerivative(u, "x", "y"))
julia> u‚Çì·µß(1.0, 2.0)
1√ó1 Array{Float64,2}:
 -0.012153494673076519
```
"""
mutable struct SecondNetDerivative
  W‚Çì::Adam
  W·µß::Adam
  Wùëß::Adam
  b‚ÇÅ::Adam
  W‚ÇÇ::Adam
  b‚ÇÇ::Adam
  W‚ÇÉ::Adam
  b‚ÇÉ::Adam

  dŒæ::AbstractArray
  dŒ∂::AbstractArray

  function SecondNetDerivative(u::NeuralNet, d‚ÇÅ, d‚ÇÇ)
    W‚Çì = u.W‚Çì
    W·µß = u.W·µß
    Wùëß = u.Wùëß
    b‚ÇÅ = u.b‚ÇÅ
    W‚ÇÇ = u.W‚ÇÇ
    b‚ÇÇ = u.b‚ÇÇ
    W‚ÇÉ = u.W‚ÇÉ
    b‚ÇÉ = u.b‚ÇÉ

    if d‚ÇÅ == "x‚ÇÅ"
        dŒæ = W‚Çì.Œ∏

    elseif d‚ÇÅ == "x‚ÇÇ"
        dŒæ = W·µß.Œ∏
    elseif d‚ÇÅ == "x‚ÇÉ"
        dŒæ = Wùëß.Œ∏
    else
        print("Must specify x or y as a string literal argument.")
    end

    if d‚ÇÇ == "x‚ÇÅ"
        dŒ∂ = W‚Çì.Œ∏

    elseif d‚ÇÇ == "x‚ÇÇ"
        dŒ∂ = W·µß.Œ∏
    elseif d‚ÇÇ == "x‚ÇÉ"
        dŒ∂ = Wùëß.Œ∏
    else
        print("Must specify x or y as a string literal argument.")
    end

    new(W‚Çì, W·µß, Wùëß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, W‚ÇÉ, b‚ÇÉ, dŒæ, dŒ∂)
  end
end

# supply a second derivative computation for each input size
function (u::SecondNetDerivative)(x)
    Œ£ = u.W‚Çì.Œ∏*x .+ u.b‚ÇÅ.Œ∏

    a = ddœÜ.(u.W‚ÇÇ.Œ∏ * œÜ.(Œ£) .+ u.b‚ÇÇ.Œ∏) .* (u.W‚ÇÇ.Œ∏ * (dœÜ.(Œ£) .* u.dŒ∂)) .* (u.W‚ÇÇ.Œ∏ * (dœÜ.(Œ£) .* u.dŒæ))

    b = dœÜ.( u.W‚ÇÇ.Œ∏ * œÜ.(Œ£) .+ u.b‚ÇÇ.Œ∏) .* (u.W‚ÇÇ.Œ∏ * (ddœÜ.(Œ£) .* u.dŒæ .* u.dŒ∂) )

    return u.W‚ÇÉ.Œ∏ * (a .+ b)
end

function (u::SecondNetDerivative)(x, y)
    Œ£ = u.W‚Çì.Œ∏*x .+ u.W·µß.Œ∏*y .+ u.b‚ÇÅ.Œ∏

    a = ddœÜ.(u.W‚ÇÇ.Œ∏ * œÜ.(Œ£) .+ u.b‚ÇÇ.Œ∏) .* (u.W‚ÇÇ.Œ∏ * (dœÜ.(Œ£) .* u.dŒ∂)) .* (u.W‚ÇÇ.Œ∏ * (dœÜ.(Œ£) .* u.dŒæ))

    b = dœÜ.( u.W‚ÇÇ.Œ∏ * œÜ.(Œ£) .+ u.b‚ÇÇ.Œ∏) .* (u.W‚ÇÇ.Œ∏ * (ddœÜ.(Œ£) .* u.dŒæ .* u.dŒ∂) )

    return u.W‚ÇÉ.Œ∏ * (a .+ b)
end

function (u::SecondNetDerivative)(x, y, z)
    Œ£ = u.W‚Çì.Œ∏*x .+ u.W·µß.Œ∏*y .+ u.Wùëß.Œ∏*z .+ u.b‚ÇÅ.Œ∏

    a = ddœÜ.(u.W‚ÇÇ.Œ∏ * œÜ.(Œ£) .+ u.b‚ÇÇ.Œ∏) .* (u.W‚ÇÇ.Œ∏ * (dœÜ.(Œ£) .* u.dŒ∂)) .* (u.W‚ÇÇ.Œ∏ * (dœÜ.(Œ£) .* u.dŒæ))

    b = dœÜ.( u.W‚ÇÇ.Œ∏ * œÜ.(Œ£) .+ u.b‚ÇÇ.Œ∏) .* (u.W‚ÇÇ.Œ∏ * (ddœÜ.(Œ£) .* u.dŒæ .* u.dŒ∂) )

    return u.W‚ÇÉ.Œ∏ * (a .+ b)
end


# given a parameter and the gradient w.r.t that parameter,
# perform an adaptive moment update
function Adam_step(P::Adam, ‚àáP::AbstractArray)
    P.‚àá = ‚àáP
    P.t += 1

    P.m = P.Œ≤‚ÇÅ * P.m + (1 - P.Œ≤‚ÇÅ) .* P.‚àá
    P.v = P.Œ≤‚ÇÇ * P.v + (1 - P.Œ≤‚ÇÇ) .* (P.‚àá).^2
    mÃÇ = P.m / (1 - (P.Œ≤‚ÇÅ).^(P.t))
    vÃÇ = P.v / (1 - (P.Œ≤‚ÇÇ).^(P.t))

    # update parameter
    P.Œ∏ .-= P.Œ± .* mÃÇ ./ (sqrt.(vÃÇ) .+ P.œµ)
end

# given a NN and its gradient, update each parameter
function Adam_update(u , ‚àáu)
    for p in u.Œ†
        Adam_step(p, ‚àáu[p.Œ∏])
    end
end


function write_params(u)
    writedlm("NN_params/Wx.csv", u.W‚Çì.Œ∏, ',')
    writedlm("NN_params/Wy.csv", u.W·µß.Œ∏, ',')
    writedlm("NN_params/Wz.csv", u.Wùëß.Œ∏, ',')
    writedlm("NN_params/b1.csv", u.b‚ÇÅ.Œ∏, ',')
    writedlm("NN_params/W2.csv", u.W‚ÇÇ.Œ∏, ',')
    writedlm("NN_params/b2.csv", u.b‚ÇÇ.Œ∏, ',')
    writedlm("NN_params/W3.csv", u.W‚ÇÉ.Œ∏, ',')
    writedlm("NN_params/b3.csv", u.b‚ÇÉ.Œ∏, ',')
end



"""
The following is just a type for a NNet with 2 hidden layers
"""
# define a few neural nets with varying number of layers, only two spatial dims
# needed
mutable struct TwoLayerNN
    W‚Çì::Adam
    W·µß::Adam
    b‚ÇÅ::Adam
    W‚ÇÇ::Adam
    b‚ÇÇ::Adam


    Œ†::Tuple
    œÄ::Flux.Params

    function TwoLayerNN(hidden_dims::Integer, out_dims::Integer, spatial_dim::Integer=2)
        W‚Çì = Adam(hidden_dims, 1)
        W·µß = Adam(hidden_dims, 1)
        b‚ÇÅ = Adam(hidden_dims, 1)
        W‚ÇÇ = Adam(out_dims, hidden_dims)
        b‚ÇÇ = Adam(out_dims, 1)

        Œ† = (W‚Çì, W·µß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ)
        œÄ = Flux.params(W‚Çì.Œ∏, W·µß.Œ∏, b‚ÇÅ.Œ∏, W‚ÇÇ.Œ∏, b‚ÇÇ.Œ∏)

        new(W‚Çì, W·µß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, Œ†, œÄ);
    end
end

# define a forward-pass rule for each different input size
(u::TwoLayerNN)(x, t) = u.W‚ÇÇ.Œ∏ * œÜ.(u.W‚Çì.Œ∏*x .+ u.W·µß.Œ∏*t .+ u.b‚ÇÅ.Œ∏) .+ u.b‚ÇÇ.Œ∏

mutable struct FirstNetDerivative_2L
    W‚Çì::Adam
    W·µß::Adam
    b‚ÇÅ::Adam
    W‚ÇÇ::Adam
    b‚ÇÇ::Adam

    dŒæ::AbstractArray

    function FirstNetDerivative_2L(u::TwoLayerNN, d)
        W‚Çì = u.W‚Çì
        W·µß = u.W·µß

        b‚ÇÅ = u.b‚ÇÅ
        W‚ÇÇ = u.W‚ÇÇ
        b‚ÇÇ = u.b‚ÇÇ


        if d == "x‚ÇÅ"
            dŒæ = W‚Çì.Œ∏

        elseif d == "x‚ÇÇ"
            dŒæ = W·µß.Œ∏
        else
            print("Must specify x‚ÇÅ. x‚ÇÇ, or x‚ÇÉ as a string literal argument.")
        end

        new(W‚Çì, W·µß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, dŒæ)
    end


end

(u::FirstNetDerivative_2L)(x, t) =  u.W‚ÇÇ.Œ∏ *( dœÜ.(u.W‚Çì.Œ∏*x .+ u.W·µß.Œ∏*t .+ u.b‚ÇÅ.Œ∏) .* u.dŒæ)





mutable struct FourLayerNN
    W‚Çì::Adam
    W·µß::Adam
    b‚ÇÅ::Adam
    W‚ÇÇ::Adam
    b‚ÇÇ::Adam
    W‚ÇÉ::Adam
    b‚ÇÉ::Adam
    W‚ÇÑ::Adam
    b‚ÇÑ::Adam

    Œ†::Tuple
    œÄ::Flux.Params

    function FourLayerNN(hidden_dims::Integer, out_dims::Integer, spatial_dim::Integer=1)
        W‚Çì = Adam(hidden_dims, 1)
        W·µß = Adam(hidden_dims, 1)
        b‚ÇÅ = Adam(hidden_dims, 1)
        W‚ÇÇ = Adam(hidden_dims, hidden_dims)
        b‚ÇÇ = Adam(hidden_dims, 1)
        W‚ÇÉ = Adam(hidden_dims, hidden_dims)
        b‚ÇÉ = Adam(hidden_dims, 1)
        W‚ÇÑ = Adam(out_dims, hidden_dims)
        b‚ÇÑ = Adam(out_dims, 1)


        Œ† = (W‚Çì, W·µß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, W‚ÇÉ, b‚ÇÉ, W‚ÇÑ, b‚ÇÑ)
        œÄ = Flux.params(W‚Çì.Œ∏, W·µß.Œ∏, b‚ÇÅ.Œ∏, W‚ÇÇ.Œ∏, b‚ÇÇ.Œ∏, W‚ÇÉ.Œ∏, b‚ÇÉ.Œ∏, W‚ÇÑ.Œ∏, b‚ÇÑ.Œ∏)

        new(W‚Çì, W·µß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, W‚ÇÉ, b‚ÇÉ,W‚ÇÑ, b‚ÇÑ, Œ†, œÄ);
    end
end

# define a forward-pass rule for each different input size
(u::FourLayerNN)(x, y) = u.W‚ÇÑ.Œ∏ * œÜ.(
                         u.W‚ÇÉ.Œ∏ * œÜ.(
                         u.W‚ÇÇ.Œ∏ * œÜ.(
                         u.W‚Çì.Œ∏*x + u.W·µß.Œ∏*y .+ u.b‚ÇÅ.Œ∏) .+ u.b‚ÇÇ.Œ∏) .+ u.b‚ÇÉ.Œ∏ ) .+ u.b‚ÇÑ.Œ∏


mutable struct FirstNetDerivative_4L
    W‚Çì::Adam
    W·µß::Adam
    b‚ÇÅ::Adam
    W‚ÇÇ::Adam
    b‚ÇÇ::Adam
    W‚ÇÉ::Adam
    b‚ÇÉ::Adam
    W‚ÇÑ::Adam
    b‚ÇÑ::Adam

    dŒæ::AbstractArray

    function FirstNetDerivative_4L(u::FourLayerNN, d)
        W‚Çì = u.W‚Çì
        W·µß = u.W·µß
        b‚ÇÅ = u.b‚ÇÅ
        W‚ÇÇ = u.W‚ÇÇ
        b‚ÇÇ = u.b‚ÇÇ
        W‚ÇÉ = u.W‚ÇÉ
        b‚ÇÉ = u.b‚ÇÉ
        W‚ÇÑ = u.W‚ÇÑ
        b‚ÇÑ = u.b‚ÇÑ

        if d == "x‚ÇÅ"
            dŒæ = W‚Çì.Œ∏
        elseif d == "x‚ÇÇ"
            dŒæ = W·µß.Œ∏
        else
            print("Must specify x‚ÇÅ. x‚ÇÇ, or x‚ÇÉ as a string literal argument.")
        end

        new(W‚Çì, W·µß, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, W‚ÇÉ, b‚ÇÉ, W‚ÇÑ, b‚ÇÑ, dŒæ)
    end
end
# supply a derivative computation for each input size


function (u::FirstNetDerivative_4L)(x, t)
    Œ£ = u.W‚Çì.Œ∏*x .+ u.W·µß.Œ∏*t .+ u.b‚ÇÅ.Œ∏
    b =  u.W‚ÇÑ.Œ∏ * ( dœÜ.(u.W‚ÇÉ.Œ∏ * œÜ.(u.W‚ÇÇ.Œ∏ * œÜ.(Œ£) .+ u.b‚ÇÇ.Œ∏) .+ u.b‚ÇÉ.Œ∏)
                 .* u.W‚ÇÉ.Œ∏ * (dœÜ.(u.W‚ÇÇ.Œ∏ * œÜ.(Œ£) .+ u.b‚ÇÇ.Œ∏)
                 .* u.W‚ÇÇ.Œ∏ * (dœÜ.(Œ£) .* u.dŒæ)))

    return b
end
