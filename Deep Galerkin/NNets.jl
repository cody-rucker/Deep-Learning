using Flux
using ForwardDiff
using Zygote

# create type to hold adaptive moment estimation parameters
"""
    Adam(N, M)
Defines a composite type to store a neural networks' vital parameters.

In addition to storing weight or bias information, this type also stores necesary
parameters for adaptive moment estimation.

# Arguments
- `N::Integer`: the desired input dimension.
- `M::Integer`: the desired output dimension.

# Examples
```jldoctest
julia> W = Adam(5, 1)
```
"""
mutable struct Adam
  Î¸::AbstractArray     # Parameter array
  âˆ‡::AbstractArray     # Gradient w.r.t Î¸
  m::AbstractArray     # First moment
  v::AbstractArray     # Second moment
  Î²â‚::Float64                   # Exp. decay first moment
  Î²â‚‚::Float64                   # Exp. decay second moment
  Î±::Float64                    # Step size
  Ïµ::Float64                    # Epsilon for stability
  t::Int                        # Time step (iteration)

  function Adam(N::Int64, M::Int64)
      Î¸ = rand(N, M)
      âˆ‡ = rand(N, M)
      m = zeros(size(Î¸))
      v = zeros(size(Î¸))
      Î²â‚= 0.9
      Î²â‚‚= 0.999
      Î± = 0.01
      Ïµ = 1e-8
      t = 0
    new(Î¸, âˆ‡, m, v, Î²â‚, Î²â‚‚, Î±, Ïµ, t)
  end

end


"""

    NeuralNet(hidden_dim, out_dim, spatial_dim)

Defines a trainable neural network which accepts an input of size `spatial_dim`
with `hidden_dim` neurons in each layer and `out_dim` entries in its output.

Weight and Bias parameters are stored as the composite Adam type to prepare the
NN for adaptive moment estimation while training.

# Arguments
- `hidden_dim::Integer`: the desired hidden dimension.
- `out_dim::Integer`: the desired output dimension.
- `spatial_dim::Integer`: the number of inputs that the NN accepts.

# Examples
```jldoctest
julia> u = NeuralNet(5,1)
julia> u(1.0, 2.0)
1Ã—1 Array{Float64,2}:
 3.338606039920986
```
"""
mutable struct NeuralNet
    Wâ‚“::Adam
    Wáµ§::Adam
    Wğ‘§::Adam
    bâ‚::Adam
    Wâ‚‚::Adam
    bâ‚‚::Adam
    Wâ‚ƒ::Adam
    bâ‚ƒ::Adam

    Î ::Tuple
    Ï€::Flux.Params

    function NeuralNet(hidden_dims::Integer, out_dims::Integer, spatial_dim::Integer=1)
        Wâ‚“ = Adam(hidden_dims, 1)
        Wáµ§ = Adam(hidden_dims, 1)
        bâ‚ = Adam(hidden_dims, 1)
        Wâ‚‚ = Adam(hidden_dims, hidden_dims)
        bâ‚‚ = Adam(hidden_dims, 1)
        Wâ‚ƒ = Adam(out_dims, hidden_dims)
        bâ‚ƒ = Adam(out_dims, 1)

        if spatial_dim == 1
            wáµ§ = Adam(1,1)
            Wğ‘§ = Adam(1,1)
            Î  = (Wâ‚“, bâ‚, Wâ‚‚, bâ‚‚, Wâ‚ƒ, bâ‚ƒ)
            Ï€ = Flux.params(Wâ‚“.Î¸, bâ‚.Î¸, Wâ‚‚.Î¸, bâ‚‚.Î¸, Wâ‚ƒ.Î¸, bâ‚ƒ.Î¸)

        elseif spatial_dim ==2
            Wáµ§ = Adam(hidden_dims, 1)
            Wğ‘§ = Adam(1,1)
            Î  = (Wâ‚“, Wáµ§, bâ‚, Wâ‚‚, bâ‚‚, Wâ‚ƒ, bâ‚ƒ)
            Ï€ = Flux.params(Wâ‚“.Î¸, Wáµ§.Î¸, bâ‚.Î¸, Wâ‚‚.Î¸, bâ‚‚.Î¸, Wâ‚ƒ.Î¸, bâ‚ƒ.Î¸)

        elseif spatial_dim == 3
            Wáµ§ = Adam(hidden_dims, 1)
            Wğ‘§ = Adam(hidden_dims, 1)
            Î  = (Wâ‚“, Wáµ§, Wğ‘§, bâ‚, Wâ‚‚, bâ‚‚, Wâ‚ƒ, bâ‚ƒ)
            Ï€ = Flux.params(Wâ‚“.Î¸, Wáµ§.Î¸, Wğ‘§.Î¸, bâ‚.Î¸, Wâ‚‚.Î¸, bâ‚‚.Î¸, Wâ‚ƒ.Î¸, bâ‚ƒ.Î¸)
        end

        new(Wâ‚“, Wáµ§, Wğ‘§, bâ‚, Wâ‚‚, bâ‚‚, Wâ‚ƒ, bâ‚ƒ, Î , Ï€);
    end
end

# define a forward-pass rule for each different input size
(u::NeuralNet)(x) = u.Wâ‚ƒ.Î¸ * Ïƒ.(
                     u.Wâ‚‚.Î¸ * Ïƒ.(
                     u.Wâ‚“.Î¸*x .+ u.bâ‚.Î¸) .+ u.bâ‚‚.Î¸) .+ u.bâ‚ƒ.Î¸

(u::NeuralNet)(x, y) = u.Wâ‚ƒ.Î¸ * Ïƒ.(
                        u.Wâ‚‚.Î¸ * Ïƒ.(
                        u.Wâ‚“.Î¸*x + u.Wáµ§.Î¸*y .+ u.bâ‚.Î¸) .+ u.bâ‚‚.Î¸) .+ u.bâ‚ƒ.Î¸ ;

(u::NeuralNet)(x, y, t) = u.Wâ‚ƒ.Î¸ * Ïƒ.(
                        u.Wâ‚‚.Î¸ * Ïƒ.(
                        u.Wâ‚“.Î¸*x + u.Wáµ§.Î¸*y + u.Wğ‘§.Î¸*t .+ u.bâ‚.Î¸) .+ u.bâ‚‚.Î¸) .+ u.bâ‚ƒ.Î¸ ;

"""

    FirstNetDerivative(u, "x")

Defines a neural network which inherits weight and bias parameters from `u` and
outputs the derivative of `u` with respect to `"x"`.

# Arguments
- `u::NeuralNet`: neural net to be differentiated.
- `"x"::String`: which variable we are differentiating with respect to. "x" and "y" are both viable inputs.

# Examples
```jldoctest
julia> u = NeuralNet(5,1)
julia> uâ‚“ = FirstNetDerivative(u, "x")
julia> uâ‚“(1.0, 2.0)
1Ã—1 Array{Float64,2}:
 0.027888484050549646
```
"""
mutable struct FirstNetDerivative
    Wâ‚“::Adam
    Wáµ§::Adam
    Wğ‘§::Adam
    bâ‚::Adam
    Wâ‚‚::Adam
    bâ‚‚::Adam
    Wâ‚ƒ::Adam
    bâ‚ƒ::Adam

    dÎ¾::AbstractArray

    function FirstNetDerivative(u::NeuralNet, d)
        Wâ‚“ = u.Wâ‚“
        Wáµ§ = u.Wáµ§
        Wğ‘§ = u.Wğ‘§
        bâ‚ = u.bâ‚
        Wâ‚‚ = u.Wâ‚‚
        bâ‚‚ = u.bâ‚‚
        Wâ‚ƒ = u.Wâ‚ƒ
        bâ‚ƒ = u.bâ‚ƒ

        if d == "xâ‚"
            dÎ¾ = Wâ‚“.Î¸

        elseif d == "xâ‚‚"
            dÎ¾ = Wáµ§.Î¸

        elseif d == "xâ‚ƒ"
            dÎ¾ = Wğ‘§.Î¸
        else
            print("Must specify xâ‚. xâ‚‚, or xâ‚ƒ as a string literal argument.")
        end

        new(Wâ‚“, Wáµ§, Wğ‘§, bâ‚, Wâ‚‚, bâ‚‚, Wâ‚ƒ, bâ‚ƒ, dÎ¾)
    end


end

# supply a derivative computation for each input size
(u::FirstNetDerivative)(x) = u.Wâ‚ƒ.Î¸ * (Ïƒ'.(u.Wâ‚‚.Î¸ * Ïƒ.( u.Wâ‚“.Î¸*x .+ u.bâ‚.Î¸) .+ u.bâ‚‚.Î¸) .*
                  (u.Wâ‚‚.Î¸ * (Ïƒ'.( u.Wâ‚“.Î¸*x .+ u.bâ‚.Î¸) .* u.dÎ¾ )))

(u::FirstNetDerivative)(x, y) = u.Wâ‚ƒ.Î¸ * (Ïƒ'.(u.Wâ‚‚.Î¸ * Ïƒ.( u.Wâ‚“.Î¸*x .+ u.Wáµ§.Î¸*y .+ u.bâ‚.Î¸) .+ u.bâ‚‚.Î¸) .*
                  (u.Wâ‚‚.Î¸ * (Ïƒ'.( u.Wâ‚“.Î¸*x .+ u.Wáµ§.Î¸*y .+ u.bâ‚.Î¸) .* u.dÎ¾ )))

(u::FirstNetDerivative)(x, y, t) = u.Wâ‚ƒ.Î¸ * (Ïƒ'.(u.Wâ‚‚.Î¸ * Ïƒ.( u.Wâ‚“.Î¸*x .+ u.Wáµ§.Î¸*y  .+ u.Wğ‘§.Î¸*t .+ u.bâ‚.Î¸) .+ u.bâ‚‚.Î¸) .*
                (u.Wâ‚‚.Î¸ * (Ïƒ'.( u.Wâ‚“.Î¸*x .+ u.Wáµ§.Î¸*y .+ u.Wğ‘§.Î¸*t .+ u.bâ‚.Î¸) .* u.dÎ¾ )))


"""

  SecondNetDerivative(u, "x", "y")

Defines a neural network which inherits weight and bias parameters from `u` and
outputs the second derivative of `u`. `"x"` and `"y"` are valid inputs allowing
you to specify any second-order derivative of `u`.

# Arguments
- `u::NeuralNet`: neural net to be differentiated.
- `"x"::String`: specifies a derivative w.r.t first input variable.
- `"y"::String`: specifies a derivative w.r.t second input variable.

# Examples
```jldoctest
julia> u = NeuralNet(5,1)
julia> uâ‚“áµ§ = SecondNetDerivative(u, "x", "y"))
julia> uâ‚“áµ§(1.0, 2.0)
1Ã—1 Array{Float64,2}:
 -0.012153494673076519
```
"""
mutable struct SecondNetDerivative
  Wâ‚“::Adam
  Wáµ§::Adam
  Wğ‘§::Adam
  bâ‚::Adam
  Wâ‚‚::Adam
  bâ‚‚::Adam
  Wâ‚ƒ::Adam
  bâ‚ƒ::Adam

  dÎ¾::AbstractArray
  dÎ¶::AbstractArray

  function SecondNetDerivative(u::NeuralNet, dâ‚, dâ‚‚)
    Wâ‚“ = u.Wâ‚“
    Wáµ§ = u.Wáµ§
    Wğ‘§ = u.Wğ‘§
    bâ‚ = u.bâ‚
    Wâ‚‚ = u.Wâ‚‚
    bâ‚‚ = u.bâ‚‚
    Wâ‚ƒ = u.Wâ‚ƒ
    bâ‚ƒ = u.bâ‚ƒ

    if dâ‚ == "xâ‚"
        dÎ¾ = Wâ‚“.Î¸

    elseif dâ‚ == "xâ‚‚"
        dÎ¾ = Wáµ§.Î¸
    elseif dâ‚ == "xâ‚ƒ"
        dÎ¾ = Wğ‘§.Î¸
    else
        print("Must specify x or y as a string literal argument.")
    end

    if dâ‚‚ == "xâ‚"
        dÎ¶ = Wâ‚“.Î¸

    elseif dâ‚‚ == "xâ‚‚"
        dÎ¶ = Wáµ§.Î¸
    elseif dâ‚‚ == "xâ‚ƒ"
        dÎ¶ = Wğ‘§.Î¸
    else
        print("Must specify x or y as a string literal argument.")
    end

    new(Wâ‚“, Wáµ§, Wğ‘§, bâ‚, Wâ‚‚, bâ‚‚, Wâ‚ƒ, bâ‚ƒ, dÎ¾, dÎ¶)
  end
end

# supply a second derivative computation for each input size
function (u::SecondNetDerivative)(x)
    Î£ = u.Wâ‚“.Î¸*x .+ u.bâ‚.Î¸

    a = Ïƒ''.(u.Wâ‚‚.Î¸ * Ïƒ.(Î£) .+ u.bâ‚‚.Î¸) .* (u.Wâ‚‚.Î¸ * (Ïƒ'.(Î£) .* u.dÎ¶)) .* (u.Wâ‚‚.Î¸ * (Ïƒ'.(Î£) .* u.dÎ¾))

    b = Ïƒ'.( u.Wâ‚‚.Î¸ * Ïƒ.(Î£) .+ u.bâ‚‚.Î¸) .* (u.Wâ‚‚.Î¸ * (Ïƒ''.(Î£) .* u.dÎ¾ .* u.dÎ¶) )

    return u.Wâ‚ƒ.Î¸ * (a .+ b)
end

function (u::SecondNetDerivative)(x, y)
    Î£ = u.Wâ‚“.Î¸*x .+ u.Wáµ§.Î¸*y .+ u.bâ‚.Î¸

    a = Ïƒ''.(u.Wâ‚‚.Î¸ * Ïƒ.(Î£) .+ u.bâ‚‚.Î¸) .* (u.Wâ‚‚.Î¸ * (Ïƒ'.(Î£) .* u.dÎ¶)) .* (u.Wâ‚‚.Î¸ * (Ïƒ'.(Î£) .* u.dÎ¾))

    b = Ïƒ'.( u.Wâ‚‚.Î¸ * Ïƒ.(Î£) .+ u.bâ‚‚.Î¸) .* (u.Wâ‚‚.Î¸ * (Ïƒ''.(Î£) .* u.dÎ¾ .* u.dÎ¶) )

    return u.Wâ‚ƒ.Î¸ * (a .+ b)
end

function (u::SecondNetDerivative)(x, y, z)
    Î£ = u.Wâ‚“.Î¸*x .+ u.Wáµ§.Î¸*y .+ u.Wğ‘§.Î¸*z .+ u.bâ‚.Î¸

    a = Ïƒ''.(u.Wâ‚‚.Î¸ * Ïƒ.(Î£) .+ u.bâ‚‚.Î¸) .* (u.Wâ‚‚.Î¸ * (Ïƒ'.(Î£) .* u.dÎ¶)) .* (u.Wâ‚‚.Î¸ * (Ïƒ'.(Î£) .* u.dÎ¾))

    b = Ïƒ'.( u.Wâ‚‚.Î¸ * Ïƒ.(Î£) .+ u.bâ‚‚.Î¸) .* (u.Wâ‚‚.Î¸ * (Ïƒ''.(Î£) .* u.dÎ¾ .* u.dÎ¶) )

    return u.Wâ‚ƒ.Î¸ * (a .+ b)
end


# given a parameter and the gradient w.r.t that parameter,
# perform an adaptive moment update
function Adam_step(P::Adam, âˆ‡P::AbstractArray)
    P.âˆ‡ = âˆ‡P
    P.t += 1

    P.m = P.Î²â‚ * P.m + (1 - P.Î²â‚) .* P.âˆ‡
    P.v = P.Î²â‚‚ * P.v + (1 - P.Î²â‚‚) .* (P.âˆ‡).^2
    mÌ‚ = P.m / (1 - (P.Î²â‚).^(P.t))
    vÌ‚ = P.v / (1 - (P.Î²â‚‚).^(P.t))

    # update parameter
    P.Î¸ .-= P.Î± .* mÌ‚ ./ (sqrt.(vÌ‚) .+ P.Ïµ)
end

# given a NN and its gradient, update each parameter
function Adam_update(u::NeuralNet, âˆ‡u)
    for p in u.Î 
        Adam_step(p, âˆ‡u[p.Î¸])
    end
end
